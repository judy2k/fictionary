#!/usr/bin/env python3

from glob import glob
from os.path import dirname, join, isdir

import fictionary
from fictionary import DICT_ALL_KEY, DICT_AMERICAN_KEY, DICT_BRITISH_KEY


# Where to load the source ispell wordlists:
SRC_ROOT = join(dirname(__file__), '..', "src")
SRC_DATA_FILE_ROOT = join(dirname(__file__), '..')

MODEL_ROOT = join(SRC_ROOT, "fictionary", "models")

assert isdir(MODEL_ROOT)

ISPELL_FILESETS = {
    DICT_ALL_KEY: glob(join(SRC_DATA_FILE_ROOT, "ispell_wordlist/*.*")),
    DICT_BRITISH_KEY: glob(join(SRC_DATA_FILE_ROOT, "ispell_wordlist/english.*"))
    + glob(join(SRC_DATA_FILE_ROOT, "ispell_wordlist/british.*")),
    DICT_AMERICAN_KEY: glob(join(SRC_DATA_FILE_ROOT, "ispell_wordlist/english.*"))
    + glob(join(SRC_DATA_FILE_ROOT, "ispell_wordlist/american.*")),
}

GLOBAL_WORDSET = set()


def create_model(files):
    """
    Read through one or more wordlist files and generate a Markov object
    representing the words and a set of all the words fed into the Markov object.

    ``files`` should be a sequence of file paths. Each file will be opened,
    and each line should contain a single word.  Words beginning with a
    capital letter or containing an apostrophe will be rejected. Each word
    is fed into a Markov object as a sequence of characters.
    """
    model = fictionary.Model()
    for path in files:
        for line in open(path, encoding='utf-8'):
            word = line.strip()
            if not line[0].isupper() and "'" not in word:
                model.feed(word)
                GLOBAL_WORDSET.add(word)
    return model


for key in [DICT_ALL_KEY, DICT_AMERICAN_KEY, DICT_BRITISH_KEY]:
    path = join(MODEL_ROOT, f"{key}.py")
    print(f"Generating {path}")
    with open(path, 'w', encoding='utf-8') as fout:
        model = create_model(ISPELL_FILESETS[key])
        print(f"""
# This module was auto-generated by the `generate` util script.

from fictionary import Model, RandomCounter
from fictionary.models.words import wordset

model = {model._code_repl()}
model._words = wordset
random_word = model.random_word

""".strip() + '\n', file=fout)

path = join(MODEL_ROOT, f"words.py")
print(f"Generating {path}")
with open(path, 'w', encoding='utf-8') as fout:
    print(f"""
# This module was auto-generated by the `generate` util script.

wordset = {GLOBAL_WORDSET!r}
    """.strip() + '\n', file=fout)